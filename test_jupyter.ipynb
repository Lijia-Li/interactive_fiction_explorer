{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import gensim\n",
    "import spacy\n",
    "import time\n",
    "import nltk.stem\n",
    "\n",
    "# download wordnet \n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Login\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "# loading model\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('./model/GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Function(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# helper function that compute the average sigma (vector difference of word pair) of a list of given conons\n",
    "def get_ave_sigma(conons):\n",
    "    sigma = 0\n",
    "    for pair in conons:\n",
    "        v, n = pair.split()\n",
    "        sigma += model.word_vec(v) - model.word_vec(n)\n",
    "    ave_sigma = (1 / len(conons)) * sigma\n",
    "    return ave_sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions and Sample Tests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get_verbs_for_noun \n",
    "    This function takes in a noun and compute the possible verbs by using word2vec model. The function uses a list of conons to computes a average vector in between noun and verbs. The vector is then used to fetch possible verbs from the model. The words returned by model are then lemmatized, compared to the top 1000 frequently use english verbs and the ones that in both set are saved. The verbs then are unioned with commonly used verbs in interactive fiction. There are currently three possible sets can be returns.\n",
    "    Please use the next cell to run for samples. Note that you can uncomment test lines in the function to inspect different verb sets.\n",
    "    ps: this algorithm is a replication of Fulda.\n",
    "\n",
    "#### get_adjectives_for_noun \n",
    "    This function takes in a noun and compute the possible adjectives by using word2vec model. The process of fetching possible adj are same as that in the get_verbs_for_noun function. The words are then lemmatized and return as a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# return a list of lemmatized verbs that the noun can afford\n",
    "def get_verbs_for_noun(noun):\n",
    "    # prepare tools\n",
    "    wnl = nltk.stem.WordNetLemmatizer()\n",
    "    conons = list(filter(None, [line.rstrip() for line in open('./word_lists/verb_noun_pair.txt')]))\n",
    "    sigma = get_ave_sigma(conons)\n",
    "\n",
    "    # list of common used verbs\n",
    "    navigation_verbs = [\"north\", \"south\", \"east\", \"west\", \"northeast\", \"southeast\", \"southwest\", \"northwest\", \"up\",\n",
    "                        \"down\", \"enter\", \"exit\"]\n",
    "    essential_manipulation_verbs = [\"get\", \"drop\", \"push\", \"pull\", \"open\", \"close\"]\n",
    "    verb_list = list(filter(None, [line.rstrip() for line in open('./word_lists/top_1000_verbs.txt')]))\n",
    "    \n",
    "    # extract words from word2vec model & append lemmatized word to list \n",
    "    model_verb = model.most_similar([sigma, noun], [], topn=10)\n",
    "    word2vec_words = []\n",
    "    for verb in model_verb:\n",
    "        word2vec_words.append(wnl.lemmatize(str(verb[0].lower())))\n",
    "    \n",
    "    # set operations\n",
    "    affordant_verbs = list(set(verb_list) & set(word2vec_words))\n",
    "    final_verbs = list(set(navigation_verbs) | set(essential_manipulation_verbs) | set(affordant_verbs))\n",
    "    \n",
    "    # -----------test lines (uncomment below four lines to view different set of verbs)-------------\n",
    "#     print(\"-\"*10, noun, \"-\"*10)\n",
    "#     print(\"word2vec words: \", word2vec_words)\n",
    "#     print(\"affordant verbs: \", affordant_verbs)\n",
    "#     print(\"final verbs: \", final_verbs)\n",
    "\n",
    "    return affordant_verbs\n",
    "\n",
    "# return a list of adjectives that describe the given noun\n",
    "def get_adjectives_for_noun(noun):\n",
    "    conons = list(filter(None, [line.rstrip() for line in open('./word_lists/noun_adj_pair.txt')]))\n",
    "#     conons = [\"knife sharp\", \"light bright\", \"ice cold\", \"fire burning\", \"desert dry\", \"sky blue\", \"night dark\",\n",
    "#                 \"rope long\"]\n",
    "    sigma = get_ave_sigma(conons)\n",
    "    model_adj = model.most_similar([sigma, noun], [], topn = 10)\n",
    "    word2vec_adj = []\n",
    "    for adj in model_adj:\n",
    "        word2vec_adj.append(adj[0])\n",
    "    return word2vec_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_nouns = [\"book\", \"sword\", \"horse\", \"key\", \"prison\"]\n",
    "\n",
    "# get_verbs_for_noun tests\n",
    "print(\"-\"*5, \"get_verbs_for_noun function tests\", \"-\"*5)\n",
    "[print(noun, \":\", get_verbs_for_noun(noun))for noun in test_nouns]\n",
    "print()\n",
    "\n",
    "# get_adjectives_for_noun tests\n",
    "print(\"-\"*5, \"get_adjectives_for_noun function tests\", \"-\"*5)\n",
    "[print(noun, \":\", get_adjectives_for_noun(noun))for noun in test_nouns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### possible_actions\n",
    "    The function take in a sentence and return a list of possible actions. \n",
    "    The algorithm uses Spacy to find nouns in the sentence. It then calls get_verbs_for_noun function to obtain a list of actions. The result first get stored in a dictionary with key being noun and value being possible actions. The function will return a list of possible actions combining keys and values of the dictionary. \n",
    "    Please use the next cell to run for samples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# return a list of possible actions by compute affordable actions on nouns in the given sentence\n",
    "def possible_actions(sentence):\n",
    "    # prepare tools\n",
    "    nlp = spacy.load('en')\n",
    "    doc = nlp(sentence)\n",
    "    wnl = nltk.stem.WordNetLemmatizer()\n",
    "    conons = list(filter(None, [line.rstrip() for line in open('./word_lists/verb_noun_pair.txt')]))\n",
    "\n",
    "    # create dictionary in the form [noun: verbs]\n",
    "    dictionary = {}\n",
    "    for chunk in doc.noun_chunks:\n",
    "        word = wnl.lemmatize(chunk.root.text)\n",
    "        if word not in dictionary:\n",
    "            dictionary[word] = get_verbs_for_noun(word)\n",
    "    \n",
    "    # loop through dictionary to creat action list\n",
    "    action_pair = []\n",
    "    for key, values in dictionary.items():\n",
    "        [action_pair.append(value + \" \" + key) for value in values]\n",
    "    return action_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# possible_actions tests\n",
    "s = \"Soon you’ll be able to send and receive money from friends and family right in Messages.\"\n",
    "s1 = \"This is an open field west of a white house, with a boarded front door. There is a small mailbox here.\"\n",
    "s2 = \"This is a forest, with trees in all directions around you.\"\n",
    "s3 = \"This is a dimly lit forest, with large trees all around.  One particularly large tree with some low branches stands here.\"\n",
    "sentences = [s, s1, s2, s3]\n",
    "\n",
    "for sentence in sentences:\n",
    "    print()\n",
    "    print(sentence)\n",
    "    print(possible_actions(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get_tools_for_verb\n",
    "    This function take in a verb and return a list of tools that can afford the verb. \n",
    "    Please use the next cell to run for samples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_tools_for_verb(verb):\n",
    "    conons = list(filter(None, [line.rstrip() for line in open('./word_lists/verb_noun_pair.txt')]))\n",
    "    sigma = get_ave_sigma(conons)\n",
    "    \n",
    "    model_tools = model.most_similar([verb], [sigma], topn=10)\n",
    "    word2vec_tools = []\n",
    "    for tool in model_tools:\n",
    "        word2vec_tools.append(tool[0])\n",
    "    return word2vec_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_verbs = [\"climb\", \"use\", \"open\", \"lift\", \"kill\", \"murder\", \"drive\", \"ride\", \"cure\", \"type\", \"sing\"]\n",
    "[print(verb, \":\", get_tools_for_verb(verb)) for verb in test_verbs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ignore main for now\n",
    "# def main():\n",
    "#     test_nouns = [\"book\", \"sword\", \"horse\", \"key\"]\n",
    "#     test_verbs = [\"climb\", \"use\", \"open\", \"lift\", \"kill\", \"murder\", \"drive\", \"ride\", \"cure\", \"type\", \"sing\"]\n",
    "#     s = \"Soon you’ll be able to send and receive money from friends and family right in Messages.\"\n",
    "#     s1 = \"This is an open field west of a white house, with a boarded front door. There is a small mailbox here.\"\n",
    "#     s2 = \"This is a forest, with trees in all directions around you.\"\n",
    "#     s3 = \"This is a dimly lit forest, with large trees all around.  One particularly large tree with some low branches stands here.\"\n",
    "#     sentences = [s, s1, s2, s3]\n",
    "    \n",
    "#     tic = time.time()\n",
    "#     [print(noun, \":\", get_verbs_for_noun(noun)) for noun in test_nouns]\n",
    "#     print(\"-\"*20)\n",
    "#     [print(verb, \":\", get_tools_for_verb(verb)) for verb in test_verbs]\n",
    "#     for sentence in sentences:\n",
    "#         print(\"-\" * 3, sentence, \"-\" * 3)\n",
    "#         pretty_print_dict(possible_actions(sentence))\n",
    "    \n",
    "#     toc = time.time()\n",
    "#     print(\"total time spend:\", toc - tic, \"s\")\n",
    "\n",
    "# if __name__ == \"__main__\": main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
